{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # An architecture similar to LeNet, except that the list of *all* actions comes in at the first fully-connected layer\n",
    "# # Useful as a discriminator for GAIL-like methods\n",
    "\n",
    "# class Net(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(5, 6, 5, padding=2)\n",
    "#         self.pool1 = nn.AvgPool2d(2, stride=2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "#         self.pool2 = nn.AvgPool2d(2, stride=2)\n",
    "#         self.fc1 = nn.Linear(576+10, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 1)\n",
    "    \n",
    "#     def forward(self, x0, x1, x2, x3, x4, a0, a1, a2, a3, a4):\n",
    "#         acts = torch.cat((a0, a1, a2, a3, a4), dim=1)\n",
    "#         z = torch.stack((x0, x1, x2, x3, x4), dim=1)\n",
    "#         z = F.relu(self.conv1(z))\n",
    "#         z = self.pool1(z)\n",
    "#         z = F.relu(self.conv2(z))\n",
    "#         z = self.pool2(z)\n",
    "#         z = F.relu(self.fc1(torch.cat((torch.flatten(z, 1), acts), dim=1)))\n",
    "#         z = F.relu(self.fc2(z))\n",
    "#         # z = self.fc3(z)\n",
    "#         return self.fc3(z), z\n",
    "\n",
    "#     def forward_stacked(self, img, act, exp):\n",
    "#         return self.forward(img[:,0,:,:], img[:,1,:,:], img[:,2,:,:], img[:,3,:,:], img[:,4,:,:], act[:,0,:], act[:,1,:], act[:,2,:], act[:,3,:], exp)\n",
    "\n",
    "# discrim = Net()\n",
    "# loss = F.mse_loss\n",
    "# sgd = opt.SGD(discrim.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_aux(dl, net, loss, opt, epochs=1):\n",
    "#     n = len(dl.dataset)\n",
    "#     ttl_err = 0\n",
    "#     ttl_ex = 0\n",
    "#     for ep in range(epochs):\n",
    "#         for _, (img, act, exp, aux) in enumerate(dl):\n",
    "#             yhat, _ = net.forward_stacked(img, act, exp)\n",
    "#             err = loss(yhat, aux)\n",
    "#             opt.zero_grad()\n",
    "#             err.backward()\n",
    "#             opt.step()\n",
    "#             ttl_err += err.item() * img.shape[0]\n",
    "#             ttl_ex += img.shape[0]\n",
    "#         print(f\"epoch: {ep:>5d}  loss: {ttl_err/ttl_ex:>7f}  processed: {ttl_ex:>5d}/{n}\")\n",
    "\n",
    "# def test_aux(dl, net, loss):\n",
    "#     with torch.no_grad():\n",
    "#         ttl_loss = 0\n",
    "#         ttl_ex = 0\n",
    "#         for _, (img, act, exp, aux) in enumerate(dl):\n",
    "#             yhat, _ = net.forward_stacked(img, act, exp)\n",
    "#             err = loss(yhat, aux).item()\n",
    "#             ttl_loss += err * img.shape[0]\n",
    "#             ttl_ex += img.shape[0]\n",
    "#         avg_loss = ttl_loss/ttl_ex\n",
    "#         print(f\"loss: {avg_loss:>7f}\")\n",
    "#         return avg_loss\n",
    "\n",
    "# def scatter_aux(dl, net, jitter = 0.05):\n",
    "#     with torch.no_grad():\n",
    "#         ys = []\n",
    "#         yhats = []\n",
    "#         for _, (img, act, exp, aux) in enumerate(dl):\n",
    "#             yhat, _ = net.forward_stacked(img, act, exp)\n",
    "#             ys.append(aux)\n",
    "#             yhats.append(yhat)\n",
    "#         ys = torch.cat(ys)\n",
    "#         yhats = torch.cat(yhats)\n",
    "#         if jitter:\n",
    "#             ys += jitter * torch.randn(ys.shape)\n",
    "#         fig = plt.plot(ys[:,0], yhats[:,0], '.')\n",
    "#         plt.ylabel('predicted')\n",
    "#         plt.xlabel('actual')\n",
    "#         # plt.legend(['x', 'y'])\n",
    "#         return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter_aux(discrim_data, discrim, jitter=0.3)\n",
    "# test_aux(discrim_data, discrim, loss)\n",
    "# train_aux(discrim_data, discrim, loss, sgd, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hires = DrivingDataset('data/hires.json.gz', name='hires')\n",
    "# hires_dl = td.DataLoader(hires, batch_size = 64)\n",
    "\n",
    "td1env = DrivingDataset('data/assist-bc-noise-2.json', name='td1env', window=6)\n",
    "print(td1env)\n",
    "td2env = DrivingDataset('data/inject-noise.json', name='td2env', window=6)\n",
    "print(td2env)\n",
    "td3env = DrivingDataset('data/assist-dagger-fix.json', name='td3env', window=6)\n",
    "print(td3env)\n",
    "td4env = DrivingDataset('data/assist-dagger-fix-2.json', name='td4env', window=6)\n",
    "print(td4env)\n",
    "tstenv = DrivingDataset('data/driving-data-w-acts-ex.json', name='tstenv', window=6)\n",
    "print(tstenv)\n",
    "train_env = td.DataLoader(td.ConcatDataset([td1env, td2env, td3env, td4env]), batch_size = 64)\n",
    "test_env = td.DataLoader(tstenv, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8d/dqwmbdms5bn20ls_13lnyycr0000gn/T/ipykernel_28673/204043860.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Decoder is like a reversed LeNet, shared for observations at t and t+1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# An architecture for learning an environment model: state encoder is like a LeNet\n",
    "# Action encoder is a small fully-connected network\n",
    "# Transition model is linear in state \\otimes action, followed by normalization\n",
    "# Decoder is like a reversed LeNet, shared for observations at t and t+1\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        state_dim = 84\n",
    "        self.fc1_act = nn.Linear(2, state_dim)\n",
    "        self.fc2_act = nn.Linear(state_dim, state_dim)\n",
    "        self.trans = nn.Linear(state_dim, state_dim)\n",
    "        self.conv1 = nn.Conv2d(4, 6, 5, padding=2)\n",
    "        self.pool1 = nn.AvgPool2d(2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.pool2 = nn.AvgPool2d(2, stride=2)\n",
    "        self.fc1 = nn.Linear(576+8, 120)\n",
    "        # self.fc1b = nn.Linear(576+10, 120)\n",
    "        self.fc2 = nn.Linear(120, state_dim)\n",
    "        # self.fc2b = nn.Linear(120+2, state_dim)\n",
    "        self.fc3 = nn.Linear(state_dim, state_dim)\n",
    "        self.fc4 = nn.Linear(state_dim, 1024)\n",
    "        # self.deconv1 = nn.ConvTranspose2d(16, 6, 5, stride=2)\n",
    "        # self.deconv2 = nn.ConvTranspose2d(6, 1, 5, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "        # self.fc3 = nn.Linear(state_dim, 120)\n",
    "        # self.fc4 = nn.Linear(120, 576)\n",
    "\n",
    "    def load_onnx(self, model):\n",
    "        wts = { t.name: torch.tensor(onnx.numpy_helper.to_array(t)) for t in model.graph.initializer }\n",
    "        self.load_state_dict(wts)\n",
    "\n",
    "    def encode_act(self, act):\n",
    "        return self.fc2_act(F.relu(self.fc1_act(act)))\n",
    "\n",
    "    def transition(self, current, act):\n",
    "        return self.trans(current)\n",
    "        # return self.trans(current * self.encode_act(act))\n",
    "        \n",
    "\n",
    "    def forward(self, x0, x1, x2, x3, a0, a1, a2, a3, a4):\n",
    "        acts = torch.cat((a0, a1, a2, a3), dim=1)\n",
    "        z = torch.stack((x0, x1, x2, x3), dim=1)\n",
    "\n",
    "        # two convolutional layers\n",
    "        z = F.relu(self.conv1(z))\n",
    "        z = self.pool1(z)\n",
    "        z = F.relu(self.conv2(z))\n",
    "        z = self.pool2(z)\n",
    "\n",
    "        # cat in the past actions, apply two fully connected layers to get the embedding\n",
    "        # for the current state\n",
    "        z = torch.flatten(z, 1)\n",
    "        embed_a = F.relu(self.fc1(torch.cat((z, acts), dim=1)))\n",
    "        embed_a = F.relu(self.fc2(embed_a))\n",
    "\n",
    "        # apply transition function to get embedding for next state\n",
    "        embed_b = self.transition(embed_a, a4)\n",
    "\n",
    "        # # cat in the past actions, apply two fully connected layers to get each embedding\n",
    "        # # note the second embedding uses one extra action\n",
    "        # z = torch.flatten(z, 1)\n",
    "        # embed_a = F.relu(self.fc1a(torch.cat((z, acts), dim=1)))\n",
    "        # embed_a = F.relu(self.fc2a(embed_a))\n",
    "        # embed_b = F.relu(self.fc1b(torch.cat((z, acts, a5), dim=1)))\n",
    "        # embed_b = F.relu(self.fc2b(embed_b))\n",
    "\n",
    "        # decode the first output\n",
    "        za = F.relu(self.fc3(embed_a))\n",
    "        za = F.relu(self.fc4(za)).reshape((-1, 32, 32))\n",
    "        # za = F.relu(self.fc4(za)).reshape((-1, 16, 6, 6))\n",
    "        # za = F.relu(self.deconv1(za))\n",
    "        # za = F.relu(self.deconv2(za)).squeeze(dim=1)\n",
    "\n",
    "        # decode the second output\n",
    "        zb = F.relu(self.fc3(embed_b))\n",
    "        zb = F.relu(self.fc4(zb)).reshape((-1, 32, 32))\n",
    "        # zb = F.relu(self.fc4(zb)).reshape((-1, 16, 6, 6))\n",
    "        # zb = F.relu(self.deconv1(zb))\n",
    "        # zb = F.relu(self.deconv2(zb)).squeeze(dim=1)\n",
    "\n",
    "        # return both outputs and both embeddings\n",
    "        return za, zb, embed_a, embed_b\n",
    "\n",
    "    def forward_stacked(self, img, act):\n",
    "        return self.forward(img[:,0,:,:], img[:,1,:,:], img[:,2,:,:], img[:,3,:,:], act[:,0,:], act[:,1,:], act[:,2,:], act[:,3,:], act[:,4,:])\n",
    "\n",
    "net = Net()\n",
    "loss = F.mse_loss\n",
    "sgd = opt.SGD(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_emodel(dl, net, loss, opt, epochs=1):\n",
    "    n = len(dl.dataset)\n",
    "    ttl_err = 0\n",
    "    ttl_ex = 0\n",
    "    for ep in range(epochs):\n",
    "        for _, (img, act, _, _, _) in enumerate(dl):\n",
    "            yhat1, yhat2, _, _ = net.forward_stacked(img[:,:-2,:,:], act)\n",
    "            err = loss(yhat1, img[:,-2,:,:])\n",
    "            err += loss(yhat2, img[:,-1,:,:])\n",
    "            opt.zero_grad()\n",
    "            err.backward()\n",
    "            opt.step()\n",
    "            ttl_err += err.item() * img.shape[0]\n",
    "            ttl_ex += img.shape[0]\n",
    "        print(f\"epoch: {ep:>5d}  loss: {ttl_err/ttl_ex:>7f}  processed: {ttl_ex:>5d}/{n}\")\n",
    "\n",
    "def test_emodel(dl, net, loss):\n",
    "    n = len(dl.dataset)\n",
    "    ttl_err = 0\n",
    "    ttl_ex = 0\n",
    "    for _, (img, act, _, _, _) in enumerate(dl):\n",
    "        yhat1, yhat2, _, _ = net.forward_stacked(img[:,:-2,:,:], act)\n",
    "        err = loss(yhat1, img[:,-2,:,:])\n",
    "        err += loss(yhat2, img[:,-1,:,:])\n",
    "        ttl_err += err.item() * img.shape[0]\n",
    "        ttl_ex += img.shape[0]\n",
    "    print(f\"loss: {ttl_err/ttl_ex:>7f} on {ttl_ex:>5d} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emodel(train_env, net, loss, sgd, epochs=10)\n",
    "test_emodel(test_env, net, loss)\n",
    "yhat1, yhat2, _, _ = net.forward_stacked(img[:,:-2,:,:], act)\n",
    "show_sa(yhat1[t,:,:].detach(), act[t,4,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 31\n",
    "randfeat = 100\n",
    "xs = torch.linspace(-1, 1, steps)\n",
    "gx, gy = torch.meshgrid(xs, xs)\n",
    "Wx = torch.randn(randfeat)*5\n",
    "Wy = torch.randn(randfeat)*5\n",
    "b = torch.rand(randfeat) * 2 * math.pi\n",
    "x, y = (.3, -.4)\n",
    "embed = torch.sin(Wx * x + Wy * y + b)\n",
    "embeds = torch.sin(Wx.reshape((-1, 1, 1)) * gx + Wy.reshape((-1, 1, 1)) * gy + b.reshape(-1, 1, 1))\n",
    "gz = torch.einsum('i,ijk', embed, embeds)/randfeat\n",
    "dd = max(xs) + 0.5 * (xs[-1] - xs[-2])\n",
    "plt.imshow(gz.t(), extent = (-dd, dd, -dd, dd), origin='lower')\n",
    "plt.plot(x, y, 'x')\n",
    "plt.colorbar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 18:29:29) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8acba6cc5b63870066fce45708faf1027105d4ca35cfc527fc6a50768a25ab7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
